{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed12c57d",
   "metadata": {},
   "source": [
    "# Insurance Fraud Detection â€“ DS3000/DS9000 Project\n",
    "**Goal:** Exploring Machine Learning Techniques for Insurance Fraud Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65678ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b771e8",
   "metadata": {},
   "source": [
    "## Import & Preprocess the dataset\n",
    "#### Please check detailed explaination in [other] files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce415497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_excel('Worksheet in Case Study question 2.xlsx', sheet_name=0)\n",
    "df=df.replace('?',np.nan)\n",
    "\n",
    "# Fill null values with 'Unknown' since having a missing value carries information in this context\n",
    "df['collision_type'] = df['collision_type'].fillna('Unknown')\n",
    "df['property_damage'] = df['property_damage'].fillna('Unknown')\n",
    "df['police_report_available'] = df['police_report_available'].fillna('Unknown')\n",
    "df['authorities_contacted'] = df['authorities_contacted'].fillna('Unknown')\n",
    "\n",
    "## Checking class balance\n",
    "df['fraud_reported'].value_counts(normalize=True)\n",
    "drop_cols = [\n",
    "        \"policy_number\", \"policy_bind_date\", \"incident_date\",\n",
    "        \"incident_location\", \"insured_zip\"\n",
    "    ]\n",
    "\n",
    "for c in drop_cols:\n",
    "    df = df.drop(columns=c)\n",
    "\n",
    "# One-hot encode all categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True) \n",
    "\n",
    "## Split and Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "y = df['fraud_reported_Y']\n",
    "X = df.drop(columns=['fraud_reported_Y'])\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "scaler = StandardScaler()\n",
    "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,        \n",
    "    random_state=42,\n",
    "    stratify=y            # maintain class balance\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34c4f2",
   "metadata": {},
   "source": [
    "## Build Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd46d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 600}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [300, 600],\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.5],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "    \"criterion\": [\"gini\", \"log_loss\"]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid=params, cv=5)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05248ec",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c1d50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faca4af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest: 75.00%\n",
      "AUC for Random Forest: 0.54\n",
      "Precision for Random Forest: 46.67%\n",
      "Recall for Random Forest: 14.29%\n",
      "F1 Score for Random Forest: 21.88%\n",
      "Average Precision Score for Random Forest: 0.28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "acc_rf = accuracy_score(y_test, y_pred)\n",
    "auc_rf = roc_auc_score(y_test, y_pred)\n",
    "precision_rf = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall_rf = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1_score_rf = f1_score(y_test, y_pred, zero_division=0)\n",
    "average_precision_score_rf = average_precision_score(y_test, y_pred)\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f'Accuracy for Random Forest: {acc_rf:.2%}')\n",
    "print(f'AUC for Random Forest: {auc_rf:.2f}')\n",
    "print(f'Precision for Random Forest: {precision_rf:.2%}')\n",
    "print(f'Recall for Random Forest: {recall_rf:.2%}')\n",
    "print(f'F1 Score for Random Forest: {f1_score_rf:.2%}')\n",
    "print(f'Average Precision Score for Random Forest: {average_precision_score_rf:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "406494d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models\\\\metrics.jsonl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "# Model I/O\n",
    "def save_model(model: Any, path: str) -> str:\n",
    "    p = Path(path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(model, p)\n",
    "    return str(p)\n",
    "\n",
    "\n",
    "# Metrics storage\n",
    "def append_metrics_jsonl(record: Dict[str, Any], path: str = \"models/metrics.jsonl\") -> str:\n",
    "    p = Path(path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with p.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "    return str(p)\n",
    "\n",
    "\n",
    "def load_metrics_jsonl(path: str = \"models/metrics.jsonl\") -> list[Dict[str, Any]]:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return []\n",
    "    records = []\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                records.append(json.loads(line))\n",
    "    return records\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "roc_auc = roc_auc_score(y_test, y_prob) if (y_prob is not None and y_test.nunique() == 2) else None\n",
    "ap = average_precision_score(y_test, y_prob) if (y_prob is not None and y_test.nunique() == 2) else None\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "metrics_record = {\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"model\": \"Random Forest\",\n",
    "        \"best_params\": grid.best_params_,\n",
    "        \"cv_best_accuracy\": float(grid.best_score_),\n",
    "        \"test_metrics\": {\n",
    "            \"accuracy\": float(acc),\n",
    "            \"precision\": float(prec),\n",
    "            \"recall\": float(rec),\n",
    "            \"f1\": float(f1),\n",
    "            \"roc_auc\": float(roc_auc) if roc_auc is not None else None,\n",
    "            \"average_precision\": float(ap) if ap is not None else None,\n",
    "            \"confusion_matrix\": cm.tolist(),\n",
    "        },\n",
    "    }\n",
    "save_model(best_model, \"models/random_forest.joblib\")\n",
    "append_metrics_jsonl(metrics_record, \"models/metrics.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
