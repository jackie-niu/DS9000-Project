# DS9000 Project â€“ Machine Learning Model Repository for Fraud Detection

This repository contains a modular machine learning workflow for training and evaluating multiple models on the **Insurance Fraud Detection dataset**.  
Each model runs independently but shares a common **preprocessing** and **metrics tracking** system.

---

## ğŸ§­ Project Structure

```
DS9000-Project/
â”‚
â”œâ”€â”€ archive/                   # Legacy code
â”‚   â””â”€â”€ Project_XGBoost.ipynb  # Original preprocessing and XGB implementation
â”‚   â””â”€â”€ Worksheet in Case Study question 2.xlsx
â”‚
â”œâ”€â”€ data/                      # Raw data files (Excel/CSV)
â”‚   â””â”€â”€ Worksheet in Case Study question 2.xlsx
â”‚
â”œâ”€â”€ models/                    # Trained models + metrics storage
â”‚   â”œâ”€â”€ xgboost_best.joblib
â”‚   â””â”€â”€ metrics.jsonl
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py          # Shared preprocessing logic (load, clean, encode, split)
â”‚   â”œâ”€â”€ utils.py               # Helper functions (save_model, append_metrics_jsonl, etc.)
â”‚   â”œâ”€â”€ leaderboard.py         # Prints leaderboard of all model runs
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ xgboost_grid.py    # XGBoost model with GridSearchCV
|       â””â”€â”€ knn.py             # KNN model with manual K loop
â”‚
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Setup

### Install dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸš€ Running a Model

All models share the same preprocessing pipeline (`src/preprocess.py`) and save results to `models/metrics.jsonl`.

### Example: Run XGBoost with Grid Search
From the **project root**, run:
```bash
python -m src.models.xgboost_grid --data "data/Worksheet in Case Study question 2.xlsx" --target fraud_reported
```

Youâ€™ll see:
- Preprocessing output (train/test fraud rates)
- Grid search results (best params)
- Final test performance (accuracy, ROC-AUC, PR-AUC)
- Saved model at: `models/xgboost_best.joblib`
- Logged metrics at: `models/metrics.jsonl`

---

## ğŸ§  Viewing the Leaderboard

After running one or more models:
```bash
python -m src.leaderboard
```

Output example:
```
=== Leaderboard ===
Sorted by: test_metrics.average_precision (desc)

timestamp                 | model        |   ACC | ROC-AUC | PR-AUC |    F1
--------------------------------------------------------------------------------
2025-11-04T02:35:46.311530 | xgboost     | 0.765 |   0.809 |   0.500 |  0.434
```

You can also sort or limit:
```bash
python -m src.leaderboard --sort roc --top 5
```

---

## â• Adding a New Model

To add another model (e.g., Logistic Regression, Random Forest, SVM):

1. **Create a new file** in `src/models/`, e.g. `logistic.py`
2. **Import**:
   ```python
   from src.preprocess import preprocess_data
   from src.utils import save_model, append_metrics_jsonl
   ```
3. **Train** your model and compute metrics (accuracy, precision, recall, F1, ROC-AUC, PR-AUC).
4. **Save results**:
   ```python
   save_model(model, "models/logistic_best.joblib")
   append_metrics_jsonl(metrics_record, "models/metrics.jsonl")
   ```
5. **Run it**:
   ```bash
   python -m src.models.logistic --data "data/Worksheet in Case Study question 2.xlsx" --target fraud_reported
   ```

Your new model will automatically appear in the leaderboard!

---

## ğŸ“¦ Model Artifacts

| File | Description |
|------|--------------|
| `models/*.joblib` | Saved trained models (load with `joblib.load`) |
| `models/metrics.jsonl` | JSON Lines file storing all model run metrics |

---

## ğŸ§© Reusing Trained Models

To make predictions later:
```python
import joblib
import pandas as pd
from src.preprocess import preprocess_data

# Load saved model
model = joblib.load("models/xgboost_best.joblib")

# Prepare new data (same format as training)
X_train, X_test, y_train, y_test = preprocess_data("data/new_claims.xlsx", target="fraud_reported")

# Predict
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]
```

---

## ğŸ§¹ Notes
- Always run from the **project root** (`python -m src.models...`) to ensure relative imports work.
- Every model automatically appends metrics to `models/metrics.jsonl`.
- You can visualize performance in notebooks or Power BI by importing that file.
