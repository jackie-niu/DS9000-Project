# DS9000 Project â€“ Machine Learning Model Repository for Fraud Detection

This repository contains a modular machine learning workflow for training and evaluating multiple models on the **Insurance Fraud Detection dataset**.  
Each model runs independently but shares a common **preprocessing** and **metrics tracking** system.

---

## ğŸ§­ Project Structure

```
DS9000-Project/
â”‚
â”œâ”€â”€ archive/                           # Legacy / exploratory notebooks
â”‚   â”œâ”€â”€ Project_CatBoost.ipynb
â”‚   â”œâ”€â”€ Project_KNN.ipynb
â”‚   â”œâ”€â”€ Project_Logistic.ipynb
â”‚   â”œâ”€â”€ Project_NN.ipynb              
â”‚   â”œâ”€â”€ Project_Random_Forest.ipynb
â”‚   â”œâ”€â”€ Project_SVM.ipynb
â”‚   â”œâ”€â”€ Project_XGBoost.ipynb         
|
â”œâ”€â”€ catboost_info/                     # Auto-generated files from running CatBoost model
|
â”œâ”€â”€ data/                              # Raw data files (Excel/CSV)
â”‚   â””â”€â”€ Worksheet in Case Study question 2.xlsx
â”‚
â”œâ”€â”€ models/                            # Saved trained models + metrics
â”‚   â”œâ”€â”€ CatBoost_best.joblib
â”‚   â”œâ”€â”€ knn_best.joblib
â”‚   â”œâ”€â”€ Logistic_Regression_best.joblib
â”‚   â”œâ”€â”€ Neural_Network_best.joblib
â”‚   â”œâ”€â”€ Random_Forest_best.joblib
â”‚   â”œâ”€â”€ svm_best.joblib
â”‚   â”œâ”€â”€ xgboost_best.joblib
â”‚   â””â”€â”€ metrics.jsonl                  # JSONL log of all experiment runs
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ leaderboard.py                 # Loads metrics.jsonl â†’ prints ranked leaderboard
â”‚   â”œâ”€â”€ preprocess.py                  # Shared preprocessing routine (loading, cleaning)
â”‚   â”œâ”€â”€ preprocess_catboost.py         # CatBoost-specific preprocessing logic
â”‚   â”œâ”€â”€ utils.py                       # save_model, metrics logging, helpers
â”‚   â”‚
â”‚   â””â”€â”€ models/                        # Individual model training scripts
â”‚       â”œâ”€â”€ knn.py                     # KNN training + evaluation
â”‚       â”œâ”€â”€ logistic.py                # Logistic Regression pipeline
â”‚       â”œâ”€â”€ mycatboost.py              # CatBoost model configuration + training
â”‚       â”œâ”€â”€ neural_network.py          # MLP-based classifier
â”‚       â”œâ”€â”€ random_forest.py           # Random Forest classifier
â”‚       â”œâ”€â”€ svm.py                     # SVM classifier with tuning
â”‚       â””â”€â”€ xgboost_grid.py            # XGBoost with GridSearchCV
â”‚
â””â”€â”€ requirements.txt           # Reproducible environment dependencies
```

---

## âš™ï¸ Setup

### Install dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸš€ Running a Model

All models share the same preprocessing pipeline (`src/preprocess.py`) and save results to `models/metrics.jsonl`.

### Example: Run XGBoost with Grid Search
From the **project root**, run:
```bash
python -m src.models.xgboost_grid --data "data/Worksheet in Case Study question 2.xlsx" --target fraud_reported
```

Youâ€™ll see:
- Preprocessing output (train/test fraud rates)
- Grid search results (best params)
- Final test performance (accuracy, ROC-AUC, PR-AUC)
- Saved model at: `models/xgboost_best.joblib`
- Logged metrics at: `models/metrics.jsonl`

---

## ğŸ§  Viewing the Leaderboard

After running one or more models:
```bash
python -m src.leaderboard
```

Output example:
```
=== Leaderboard ===
Sorted by: test_metrics.average_precision (desc)

timestamp                 | model        |   ACC | ROC-AUC | PR-AUC |    F1
--------------------------------------------------------------------------------
2025-11-04T02:35:46.311530 | xgboost     | 0.765 |   0.809 |   0.500 |  0.434
```

You can also sort or limit:
```bash
python -m src.leaderboard --sort roc --top 5
```

---

## â• Adding a New Model

To add another model (e.g., Logistic Regression, Random Forest, SVM):

1. **Create a new file** in `src/models/`, e.g. `logistic.py`
2. **Import**:
   ```python
   from src.preprocess import preprocess_data
   from src.utils import save_model, append_metrics_jsonl
   ```
3. **Train** your model and compute metrics (accuracy, precision, recall, F1, ROC-AUC, PR-AUC).
4. **Save results**:
   ```python
   save_model(model, "models/logistic_best.joblib")
   append_metrics_jsonl(metrics_record, "models/metrics.jsonl")
   ```
5. **Run it**:
   ```bash
   python -m src.models.logistic --data "data/Worksheet in Case Study question 2.xlsx" --target fraud_reported
   ```

Your new model will automatically appear in the leaderboard!

---

## ğŸ“¦ Model Artifacts

| File | Description |
|------|--------------|
| `models/*.joblib` | Saved trained models (load with `joblib.load`) |
| `models/metrics.jsonl` | JSON Lines file storing all model run metrics |

---

## ğŸ§© Reusing Trained Models

To make predictions later:
```python
import joblib
import pandas as pd
from src.preprocess import preprocess_data

# Load saved model
model = joblib.load("models/xgboost_best.joblib")

# Prepare new data (same format as training)
X_train, X_test, y_train, y_test = preprocess_data("data/new_claims.xlsx", target="fraud_reported")

# Predict
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]
```

---

## ğŸ§¹ Notes
- Always run from the **project root** (`python -m src.models...`) to ensure relative imports work.
- Every model automatically appends metrics to `models/metrics.jsonl`.
- You can visualize performance in notebooks or Power BI by importing that file.
